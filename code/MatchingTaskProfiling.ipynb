{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile each matching task of the setting along the five dimensions:\n",
    "1. Schema Complexity : number of relevant attributes\n",
    "2. Sparsity : ratio of missing values to all attribute values of all relevant attributes\n",
    "3. Size : size of training and validation set\n",
    "4. Corner Cases : apply the corner cases with optimal threshold heuristic and calculate (#false_positives + #false_negatives)/ matching_pairs\n",
    "5. Textuality : average length value in words of top relevant attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataset pair 2_1\n",
      "Current dataset pair 2_3\n",
      "Current dataset pair 2_4\n",
      "Current dataset pair 2_5\n",
      "Current dataset pair 3_1\n",
      "Current dataset pair 4_1\n",
      "Current dataset pair 4_5\n",
      "Current dataset pair 4_3\n",
      "Current dataset pair 5_1\n",
      "Current dataset pair 5_3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset pair</th>\n",
       "      <th>F1-xval_all_attr</th>\n",
       "      <th>Relevant Attributes</th>\n",
       "      <th>Top Features</th>\n",
       "      <th>Schema Complexity</th>\n",
       "      <th>Textuality</th>\n",
       "      <th>Sparsity</th>\n",
       "      <th>Size</th>\n",
       "      <th>Match#</th>\n",
       "      <th>Corner Cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2_1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>[album*, length*, title, year, number]</td>\n",
       "      <td>[album_lev, album_token_jaccard, album_relaxed...</td>\n",
       "      <td>5</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.08</td>\n",
       "      <td>38270</td>\n",
       "      <td>1984</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_3</td>\n",
       "      <td>0.85</td>\n",
       "      <td>[title*, number*, length, year, language]</td>\n",
       "      <td>[title_relaxed_jaccard, title_jaccard, title_c...</td>\n",
       "      <td>5</td>\n",
       "      <td>7.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>39260</td>\n",
       "      <td>2030</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2_4</td>\n",
       "      <td>0.98</td>\n",
       "      <td>[length*, album*, title, number]</td>\n",
       "      <td>[length_sim, album_lev]</td>\n",
       "      <td>4</td>\n",
       "      <td>4.26</td>\n",
       "      <td>0.05</td>\n",
       "      <td>38608</td>\n",
       "      <td>1244</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_5</td>\n",
       "      <td>0.98</td>\n",
       "      <td>[album*, length*, title, number]</td>\n",
       "      <td>[album_relaxed_jaccard, album_lev, album_token...</td>\n",
       "      <td>4</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.06</td>\n",
       "      <td>38278</td>\n",
       "      <td>1284</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3_1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>[title*, artist, length]</td>\n",
       "      <td>[title_relaxed_jaccard]</td>\n",
       "      <td>3</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.05</td>\n",
       "      <td>38270</td>\n",
       "      <td>1208</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4_1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>[length*, album*, artist, title]</td>\n",
       "      <td>[length_sim, length_num_equal, album_lev]</td>\n",
       "      <td>4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.06</td>\n",
       "      <td>38270</td>\n",
       "      <td>1216</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4_3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>[artist*, title*, number, length, language]</td>\n",
       "      <td>[artist_lev, title_containment, title_jaccard]</td>\n",
       "      <td>5</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0.06</td>\n",
       "      <td>39246</td>\n",
       "      <td>2002</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4_5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>[title*, album*, length, artist, number]</td>\n",
       "      <td>[title_relaxed_jaccard, album_lev]</td>\n",
       "      <td>5</td>\n",
       "      <td>7.54</td>\n",
       "      <td>0.06</td>\n",
       "      <td>38261</td>\n",
       "      <td>1980</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5_1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>[album*, length*, artist, title, year]</td>\n",
       "      <td>[album_relaxed_jaccard, album_lev, album_token...</td>\n",
       "      <td>5</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.07</td>\n",
       "      <td>38186</td>\n",
       "      <td>1969</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5_3</td>\n",
       "      <td>0.96</td>\n",
       "      <td>[title*, artist*, number, length]</td>\n",
       "      <td>[title_jaccard, title_relaxed_jaccard, title_c...</td>\n",
       "      <td>4</td>\n",
       "      <td>7.76</td>\n",
       "      <td>0.06</td>\n",
       "      <td>39260</td>\n",
       "      <td>1262</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset pair F1-xval_all_attr                          Relevant Attributes  \\\n",
       "0          2_1             0.98       [album*, length*, title, year, number]   \n",
       "1          2_3             0.85    [title*, number*, length, year, language]   \n",
       "2          2_4             0.98             [length*, album*, title, number]   \n",
       "3          2_5             0.98             [album*, length*, title, number]   \n",
       "4          3_1             0.98                     [title*, artist, length]   \n",
       "5          4_1             0.99             [length*, album*, artist, title]   \n",
       "7          4_3             0.97  [artist*, title*, number, length, language]   \n",
       "6          4_5             0.99     [title*, album*, length, artist, number]   \n",
       "8          5_1             0.99       [album*, length*, artist, title, year]   \n",
       "9          5_3             0.96            [title*, artist*, number, length]   \n",
       "\n",
       "                                        Top Features Schema Complexity  \\\n",
       "0  [album_lev, album_token_jaccard, album_relaxed...                 5   \n",
       "1  [title_relaxed_jaccard, title_jaccard, title_c...                 5   \n",
       "2                            [length_sim, album_lev]                 4   \n",
       "3  [album_relaxed_jaccard, album_lev, album_token...                 4   \n",
       "4                            [title_relaxed_jaccard]                 3   \n",
       "5          [length_sim, length_num_equal, album_lev]                 4   \n",
       "7     [artist_lev, title_containment, title_jaccard]                 5   \n",
       "6                 [title_relaxed_jaccard, album_lev]                 5   \n",
       "8  [album_relaxed_jaccard, album_lev, album_token...                 5   \n",
       "9  [title_jaccard, title_relaxed_jaccard, title_c...                 4   \n",
       "\n",
       "  Textuality Sparsity   Size Match# Corner Cases  \n",
       "0       3.77     0.08  38270   1984         0.22  \n",
       "1       7.24     0.12  39260   2030         0.39  \n",
       "2       4.26     0.05  38608   1244         0.42  \n",
       "3       3.77     0.06  38278   1284         0.19  \n",
       "4       7.49     0.05  38270   1208         0.08  \n",
       "5        4.2     0.06  38270   1216         0.16  \n",
       "7       7.81     0.06  39246   2002         0.18  \n",
       "6       7.54     0.06  38261   1980         0.11  \n",
       "8       3.74     0.07  38186   1969          0.2  \n",
       "9       7.76     0.06  39260   1262         0.16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_matching_task_profile_info()\n",
    "\n",
    "importantProfilingDimensions()\n",
    "#display(matching_tasks_profiling)\n",
    "#matching_tasks_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import*\n",
    "import os\n",
    "import os.path as path\n",
    "from learningutils import *\n",
    "from sklearn import tree\n",
    "from matching_task import *\n",
    "import time\n",
    "import glob\n",
    "\n",
    "\n",
    "matching_tasks_summary = pd.DataFrame(columns=['Dataset', '#records_source', '#records_target', 'count_record_pairs', '#match', '#non-match',\n",
    "                                               'count_attr','#short_string_attr', '#long_string_attr', '#numeric_attr','avg_density_all'])\n",
    "matching_tasks_baseline_rf_results = pd.DataFrame(columns=['Dataset', 'precision','recall','f1','f1_std','proba_scores',\n",
    "                                                          'proba_scores_std','x-val f1','x-val f1 sigma'])\n",
    "matching_tasks_baseline_svm_results = pd.DataFrame(columns=['Dataset', 'precision','recall','f1','f1_std','proba_scores',\n",
    "                                                      'proba_scores_std','x-val f1','x-val f1 sigma'])\n",
    "\n",
    "matching_tasks_profiling = pd.DataFrame(columns=['Dataset','F1_xval_max', 'F1_xval_top_matching_relevant_features', \n",
    "                                                 'matching_relevant_features', \n",
    "   'matching_relevant_attributes','matching_relevant_attributes_density','matching_relevant_attributes_count',\n",
    " 'matching_relevant_attributes_datatypes','top_matching_relevant_features','top_relevant_attributes', \n",
    " 'top_relevant_attributes_count','top_relevant_attributes_datatypes', 'top_relevant_attributes_density',\n",
    "'avg_length_tokens_top_relevant_attributes','avg_length_words_top_relevant_attributes','corner_cases_top_matching_relevant_features'])\n",
    " \n",
    "# Use the flags below to indicate which results should be calculates\n",
    "summaryFeatures=True\n",
    "baselineResults=False\n",
    "profilingFeatures = True                                                           \n",
    "\n",
    "\n",
    "main_path = '../datasets/musicbrainz20K/'\n",
    "source_folder=\"sources\"\n",
    "fv_folder = \"feature_vector_files/\"\n",
    "\n",
    "#add the correct separators of the source sets and the gold standard\n",
    "sep_for_source_files= ','\n",
    "gs_sep = ','\n",
    "train_test_val=False # otherwise nested x-validation for baseline experiments\n",
    "fv_name_split = \"_\"\n",
    "#change for allowing multithreading\n",
    "threads=-1\n",
    "\n",
    "def get_matching_task_profile_info():\n",
    "     \n",
    "    dat_counter = 0\n",
    "    for f in glob.glob(main_path+fv_folder+\"/*\"):\n",
    "        dataset_name = f.split(\"/\")[-1].replace(\".csv\",\"\")\n",
    "        print(\"Current dataset pair %s\" %dataset_name)\n",
    "        ds1_name = dataset_name.split(fv_name_split)[0]\n",
    "        ds2_name = dataset_name.split(fv_name_split)[1]\n",
    "        \n",
    "        feature_vector = pd.read_csv(f)\n",
    "       \n",
    "\n",
    "        gs = feature_vector[['source_id','target_id','label']].copy()\n",
    "        gs.rename(columns={'label':'matching'}, inplace=True)\n",
    "\n",
    "        ds1= pd.read_csv(main_path+source_folder+\"/\"+ds1_name+\".csv\", sep =sep_for_source_files, engine='python')\n",
    "        ds2= pd.read_csv(main_path+source_folder+\"/\"+ds2_name+\".csv\", sep =sep_for_source_files, engine='python')\n",
    "        \n",
    "        #ds1.drop(columns=['cluster_id'], inplace=True)\n",
    "\n",
    "        ds1.rename(columns={'id':'subject_id'}, inplace=True)\n",
    "        #ds2.drop(columns=['cluster_id'], inplace=True)\n",
    "        ds2.rename(columns={'id':'subject_id'}, inplace=True)\n",
    "\n",
    "        if not ds1.empty and not ds2.empty and not gs.empty:\n",
    "            ds1['subject_id'] = ds1['subject_id'].apply(str)\n",
    "            ds2['subject_id'] = ds2['subject_id'].apply(str)\n",
    "\n",
    "\n",
    "            gs['source_id'] = gs['source_id'].apply(str)\n",
    "            gs['target_id'] = gs['target_id'].apply(str)\n",
    "\n",
    "            common_attributes = [value for value in ds1.columns if (value in ds2.columns and value!='subject_id')] \n",
    "            matching_task = MatchingTask(ds1, ds2, gs, feature_vector, common_attributes)\n",
    "\n",
    "            if (summaryFeatures):\n",
    "                matching_task.getSummaryFeatures()\n",
    "                #correspondes features\n",
    "                summary_features = matching_task.dict_summary\n",
    "                summary_features['Dataset'] = dataset_name\n",
    "\n",
    "                for key in matching_tasks_summary.columns:\n",
    "                    matching_tasks_summary.loc[dat_counter, key] = summary_features.get(key)\n",
    "\n",
    "\n",
    "            if (baselineResults):\n",
    "                # get baseline results\n",
    "                if (train_test_val): \n",
    "                    print(\"Evaluation with train_validation_test split\")\n",
    "                    matching_task.getSplitValidationResults(model=\"linear\")\n",
    "                    matching_task.getSplitValidationResults(model=\"non-linear\")\n",
    "                else:\n",
    "                    print(\"Evaluation with Nested-X-Validation (no splits will be considered) - slow for large tasks\")\n",
    "                    matching_task.getNestedXValidationResults(model=\"linear\")\n",
    "                    matching_task.getNestedXValidationResults(model=\"non-linear\")\n",
    "\n",
    "                #linear model results\n",
    "                linear_results = matching_task.dict_linear_results\n",
    "                linear_results['Dataset'] = dataset_name\n",
    "                for key in linear_results:\n",
    "                    matching_tasks_baseline_svm_results.loc[dat_counter, key] = linear_results.get(key)\n",
    "                #non-linear model results\n",
    "                non_linear_results = matching_task.dict_non_linear_results\n",
    "                non_linear_results['Dataset'] = dataset_name\n",
    "                for key in non_linear_results:\n",
    "                    matching_tasks_baseline_rf_results.loc[dat_counter, key] = non_linear_results.get(key)\n",
    "\n",
    "            if(profilingFeatures):\n",
    "                matching_task.getProfilingFeatures()\n",
    "                ident_features_profile =  matching_task.dict_profiling_features\n",
    "                ident_features_profile['Dataset'] = dataset_name\n",
    "                for key in matching_tasks_profiling.columns:\n",
    "                    matching_tasks_profiling.loc[dat_counter,key] = ident_features_profile.get(key)                                  \n",
    "\n",
    "\n",
    "            dat_counter+=1\n",
    "\n",
    "    \n",
    "def displaySaveResults(): \n",
    "    timestr = time.strftime(\"%m%d_%H%M%S\")\n",
    "    if (profilingFeatures):\n",
    "        display(matching_tasks_summary)\n",
    "        matching_tasks_summary.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_profiling.csv', index=False)\n",
    "\n",
    "    if (baselineResults):\n",
    "        display(matching_tasks_svm_results)\n",
    "        display(matching_tasks_rf_results)\n",
    "        matching_tasks_rf_results.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_RF_results.csv', index=False)\n",
    "        matching_tasks_svm_results.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_SVM_results.csv', index=False)\n",
    "\n",
    "    if(profilingFeatures):\n",
    "        display(matching_tasks_profiling)\n",
    "        matching_tasks_profiling.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_profiling_features_summary.csv', index=False)\n",
    "\n",
    "      \n",
    "def importantProfilingDimensions():\n",
    "    profiling_dimensions = pd.DataFrame(columns=['Dataset'])\n",
    "    profiling_dimensions['Dataset'] = matching_tasks_summary.Dataset\n",
    "    profiling_dimensions['Size'] = matching_tasks_summary.count_record_pairs\n",
    "    profiling_dimensions['Match#'] = matching_tasks_summary['#match']\n",
    "    profiling_dimensions = pd.merge(profiling_dimensions, matching_tasks_profiling)\n",
    "    \n",
    "    for index, row in profiling_dimensions.iterrows():\n",
    "        relev_attr = row['matching_relevant_attributes']\n",
    "        \n",
    "        top_relev_attr = row['top_relevant_attributes']\n",
    "        \n",
    "        format_relev_attr = []\n",
    "        for ra in relev_attr:\n",
    "            if ra in top_relev_attr: format_relev_attr.append(ra+\"*\")\n",
    "            else: format_relev_attr.append(ra)\n",
    "        profiling_dimensions.loc[index,'matching_relevant_attributes']=format_relev_attr\n",
    "        \n",
    "    columns = ['Dataset pair', 'F1-xval_all_attr', 'Relevant Attributes', 'Top Features', 'Schema Complexity', 'Textuality', 'Sparsity', 'Size', 'Match#', 'Corner Cases']\n",
    "    profiling_dimensions.rename(columns={'Dataset':columns[0], 'F1_xval_max':columns[1],'matching_relevant_attributes':columns[2], 'top_matching_relevant_features':columns[3],\n",
    "                                       'matching_relevant_attributes_count':columns[4], 'avg_length_words_top_relevant_attributes':columns[5],\n",
    "                                        'matching_relevant_attributes_density':columns[6], 'corner_cases_top_matching_relevant_features':columns[9]}, inplace=True)\n",
    "    \n",
    "    profiling_dimensions= profiling_dimensions[columns]\n",
    "    profiling_dimensions['Sparsity'] = 1-profiling_dimensions['Sparsity']\n",
    "    profiling_dimensions.sort_values(by=['Dataset pair'], inplace=True)  \n",
    "    display(profiling_dimensions)\n",
    "    profiling_dimensions.to_csv(main_path+\"profiling.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
